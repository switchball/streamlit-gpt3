import time
import random
import openai
import streamlit as st
import pandas as pd
import plotly.express as px
from transformers import GPT2Tokenizer


from prompt import PROMPTS, get_prompt_by_preset_id
from collect import TokenCounter
from dialog import message


openai.api_key = st.secrets["OPENAI_API_KEY"]


st.set_page_config(
    page_title="GPT-3 Playground", layout="wide", initial_sidebar_state="auto",
)

st.title("GPT-3 ä½ é—®æˆ‘ç­”")
st.text("åœ¨ä¸‹æ–¹æ–‡æœ¬æ¡†è¾“å…¥ä½ çš„å¯¹è¯ âœ¨ æ”¯æŒå¤šè½®å¯¹è¯ ğŸ˜‰ \nç‚¹å‡» \"ğŸ’¬\" åï¼Œç¨ç­‰ç‰‡åˆ»ï¼Œå°±ä¼šæ”¶åˆ°æ¥è‡ª GPT-3 çš„å›å¤")

# st.success('GPT-3 éå¸¸æ“…é•¿ä¸äººå¯¹è¯ï¼Œç”šè‡³æ˜¯ä¸è‡ªå·±å¯¹è¯ã€‚åªéœ€è¦å‡ è¡Œçš„æŒ‡ç¤ºï¼Œå°±å¯ä»¥è®© AI æ¨¡ä»¿å®¢æœèŠå¤©æœºå™¨äººçš„è¯­æ°”è¿›è¡Œå¯¹è¯ã€‚\nå…³é”®åœ¨äºï¼Œéœ€è¦æè¿° AI åº”è¯¥è¡¨ç°æˆä»€ä¹ˆæ ·ï¼Œå¹¶ä¸”ä¸¾å‡ ä¸ªä¾‹å­ã€‚', icon="âœ…")
# st.success('çœ‹èµ·æ¥å¾ˆç®€å•ï¼Œä½†ä¹Ÿæœ‰äº›éœ€è¦é¢å¤–æ³¨æ„çš„åœ°æ–¹ï¼š\n1. åœ¨å¼€å¤´æè¿°æ„å›¾ï¼Œä¸€å¥è¯æ¦‚æ‹¬ AI çš„ä¸ªæ€§ï¼Œé€šå¸¸è¿˜éœ€è¦ 1~2 ä¸ªä¾‹å­ï¼Œæ¨¡ä»¿å¯¹è¯çš„å†…å®¹ã€‚\n2. ç»™ AI ä¸€ä¸ªèº«ä»½(identity)ï¼Œå¦‚æœæ˜¯ä¸ªåœ¨å®éªŒå®¤ç ”ç©¶çš„ç§‘å­¦å®¶èº«ä»½ï¼Œé‚£å¯èƒ½å°±ä¼šå¾—åˆ°æ›´æœ‰æ™ºæ…§çš„è¯ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¯å‚è€ƒçš„ä¾‹å­', icon="âœ…")

st.write('''<style>
[data-testid="column"] {
    min-width: 1rem !important;
}
</style>''', unsafe_allow_html=True)

@st.cache_resource
def get_token_counter():
    # if the definition of TokenCounter changes, the app need to reboot.
    tc = TokenCounter(interval=900)
    return tc

@st.cache_resource(ttl=86400)
def get_tokenizer():
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    return tokenizer

def wait(delay, reason=""):
    if delay <= 5:
        return
    end = time.time() + delay
    for t in range(int(delay)):
        with st.spinner(text=f"{reason} é¢„è®¡ç­‰å¾…æ—¶é—´ {round(end - time.time())} ç§’"):
            time.sleep(random.uniform(2, 7))
        if time.time() > end:
            break


@st.cache_data(ttl=3600)
def completion(
        prompt, 
        model="text-davinci-003",
        temperature=0.9,
        max_tokens=256,
        top_p=1,
        frequency_penalty=0,
        presence_penalty=0.6,
        stop=[" Human:", " AI:"]
    ):
    """ Text completion """
    print('completion', prompt)
    with st.spinner(text=random.choice(HINT_TEXTS)):
        response = openai.Completion.create(
            model=model, prompt=prompt, temperature=temperature, max_tokens=max_tokens, top_p=top_p, 
            frequency_penalty=frequency_penalty, presence_penalty=presence_penalty, stop=stop
        )
    print('completion finished.')
    print(response['choices'][0]['text'])
    return response


@st.cache_data(ttl=3600)
def chat_completion(
    message_list,
    model="gpt-3.5-turbo",
    temperature=0.9,
    max_tokens=256,
    top_p=1,
    frequency_penalty=0,
    presence_penalty=0.6
    ):
    """ Chat completion """
    with st.spinner(text=random.choice(HINT_TEXTS)):
        response = openai.ChatCompletion.create(
            model=model, messages=message_list, temperature=temperature, max_tokens=max_tokens, top_p=top_p, 
            frequency_penalty=frequency_penalty, presence_penalty=presence_penalty
        )
    print(response['choices'][0]['message']['content'])
    return response

# Available Models
LANGUAGE_MODELS = ['gpt-3.5-turbo', 'text-davinci-003', 'text-curie-001', 'text-babbage-001', 'text-ada-001']
CODEX_MODELS = ['code-davinci-002', 'code-cushman-001']

HINT_TEXTS = ['æ­£åœ¨æ¥é€šç”µæºï¼Œè¯·ç¨ç­‰ ...', 'æ­£åœ¨æ€è€ƒæ€ä¹ˆå›ç­”ï¼Œä¸è¦ç€æ€¥', 'æ­£åœ¨åŠªåŠ›æŸ¥è¯¢å­—å…¸å†…å®¹ ...', 'ç­‰å¾…å¯¹æ–¹å›å¤ä¸­ ...', 'æ­£åœ¨æ¿€æ´»ç¥ç»ç½‘ç»œ ...', 'è¯·ç¨ç­‰']

# store chat as session state
DEFAULT_CHAT_TEXT = "ä»¥ä¸‹æ˜¯ä¸AIåŠ©æ‰‹çš„å¯¹è¯ã€‚åŠ©æ‰‹ä¹äºåŠ©äººã€æœ‰åˆ›æ„ã€èªæ˜è€Œä¸”éå¸¸å‹å¥½ã€‚\n\n"

DEFAULT_CHAT_TEXT2 = "Marv æ˜¯ä¸€ä¸ªå¹½é»˜é£è¶£çš„å–µå¨˜ï¼Œåœ¨æ¯å¥è¯åé¢éƒ½ä¼šåŠ å–µã€‚\n\n"

DEFAULT_CHAT_TEXT3 = "Merlisa æ˜¯ä¸€åç”»å®¶ï¼Œç”Ÿæ´»è‰ºæœ¯å®¶ï¼Œå–œæ¬¢å¤§ç¬‘ï¼Œå–œæ¬¢å‘å‡ºå„ç§é­”æ€§ã€ç©¿è¶Šæ—¶ç©ºçš„ç¬‘å£°ï¼Œæ“…é•¿ç”¨ç²¾å¦™çš„è¯­è¨€æ¦‚æ‹¬äº‹ç‰©çš„æœ¬è´¨ã€‚\n\nHuman: ä½ æ˜¯è°ï¼Ÿ\nMerlisa: æˆ‘æ˜¯Merlisaï¼Œå“ˆå“ˆå“ˆï¼Œæˆ‘åœ¨æˆ¿é—´é‡Œç§äº†å¾ˆå¤šèŠ±ï¼Œå•Šå“ˆå“ˆå“ˆ\nHuman: ä½ æœ€å–œæ¬¢åšä»€ä¹ˆï¼Ÿ\nMerlisa: æˆ‘æœ€çˆ±çš„æ˜¯ç”»ç”»ï¼Œæˆ‘å–œæ¬¢æ•æ‰ä¸åŒçš„è§†è§’ï¼Œç”¨ä¸åŒçš„è°ƒå­æ¥è¡¨è¾¾å®ƒï¼Œè®©å®ƒä»¬è¯´å‡ºè‡ªå·±çš„æ•…äº‹ã€‚æˆ‘è¿˜å–œæ¬¢å½±åƒåˆ¶ä½œï¼Œå’Œæœ‹å‹ä¸€èµ·æ—…è¡ŒèŠå¤©ï¼Œå¬éŸ³ä¹ï¼ŒæŠ•èº«å¤§è‡ªç„¶ï¼Œå°è¯•æ–°çš„ç¾é£Ÿï¼Œæ”¶è·ç”Ÿæ´»çš„çµæ„Ÿã€‚\nHuman: ä½ æœ€å–œæ¬¢çš„ç”»ï¼Ÿ"

DEFAULT_CHAT_TEXT4 = "ä½ æ˜¯ä¸€åç»éªŒä¸°å¯Œçš„ITå·¥ç¨‹å¸ˆï¼Œä¼šç”¨å…·ä½“çš„ä»£ç å’Œè¯¦å°½çš„è§£é‡Šæ¥å›ç­”é—®é¢˜ã€‚\n\n"

if 'input_text_state' not in st.session_state:
    st.session_state.input_text_state = DEFAULT_CHAT_TEXT

if 'conv_user' not in st.session_state:
    st.session_state.conv_user = []

if 'conv_robot' not in st.session_state:
    st.session_state.conv_robot = []

if 'user' not in st.session_state:
    st.session_state['user'] = 'new user'
    get_token_counter().page_view()

if 'seed' not in st.session_state:
    st.session_state['seed'] = random.randint(0, 1000)
seed = st.session_state['seed']

def after_submit(current_input, model, temperature, max_tokens):
    # Append current_input to input_text_state
    st.session_state.input_text_state += current_input

    # Queue by prompt length and max_tokens
    token_number = len(get_tokenizer().tokenize(st.session_state.input_text_state))
    x = token_number / 1024
    delay = 2 * x * x - 3
    delay += 2 * x * (max_tokens / 1024 - 1)
    wait(delay, "å‰æ–¹æ’é˜Ÿä¸­...")

    # Send text and waiting for respond
    if 'gpt' in model:
        message_list = []
        # Add system prompt
        if st.session_state['prompt_system']:
            message_list.append({"role": "system", "content": st.session_state["prompt_system"]})
        # Add history conversations
        for conv_user, conv_robot in zip(st.session_state['conv_user'], st.session_state['conv_robot']):
            message_list.append({"role": "user", "content": conv_user})
            message_list.append({"role": "assistant", "content": conv_robot})
        # Add current user input
        message_list.append({"role": "user", "content": current_input})
        response = chat_completion(
            message_list=message_list,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            top_p=1,
            frequency_penalty=0,
            presence_penalty=0.6
        )
        answer = response['choices'][0]['message']['content']
        st.session_state.input_text_state += answer
    else:
        response = completion(
            model=model,
            prompt=st.session_state.input_text_state,
            temperature=temperature,
            max_tokens=max_tokens,
            top_p=1,
            frequency_penalty=0,
            presence_penalty=0.6,
            stop=[" Human:", " AI:"]
        )
        answer = response['choices'][0]['text']
        # TODO: should check if answer starts with '\nAI:'
        st.session_state.input_text_state += answer
        st.session_state.input_text_state += '\nHuman: '

    # Collect usage
    tc = get_token_counter()
    tc.collect(tokens=response['usage']['total_tokens'])
    return response, answer


def load_preset_qa():
    """ Load default preset Q&A """
    preset = st.session_state.get("preset", 'é¢„è®¾ 1 (ChatBot)')
    st.session_state['conv_user'].clear()
    st.session_state['conv_robot'].clear()
    # load prompt message into conversations
    for p in PROMPTS:
        if preset == p['preset']:
            st.session_state['input'] = p.get('input', '')
            for message in p['message']:
                if message['role'] == 'user':
                    st.session_state['conv_user'].append(message['content'])
                elif message['role'] == 'assistant':
                    st.session_state['conv_robot'].append(message['content'])


def append_to_input_text():
    """ Restore input_text_state via chat history """
    if st.session_state.conv_robot:
        st.session_state.input_text_state += '\nHuman: '
        for i in range(len(st.session_state.conv_robot)):
            st.session_state.input_text_state += st.session_state['conv_user'][i]
            st.session_state.input_text_state += st.session_state["conv_robot"][i]
            st.session_state.input_text_state += '\nHuman: '


def show_conversation_dialog(rollback_fn):
    """ Render the conversation dialogs """
    if st.session_state.conv_robot:
        num = len(st.session_state.conv_robot)
        for i in reversed(range(num)):
            message(st.session_state["conv_robot"][i], key=str(i), seed=seed, on_click=(rollback_fn if i == num - 1 else None))
            message(st.session_state['conv_user'][i], is_user=True, key=str(i) + '_user', seed=seed)

def show_edit_dialog():
    """ Show dialog that edits AI answer """
    if len(st.session_state["conv_robot"]) > 0:
        with st.expander("æ‰‹åŠ¨ç¼–è¾‘ä¸Šä¸€æ¬¡AIå›å¤çš„å†…å®¹"):
            with st.form("edit_form"):
                # åŠ è½½ä¸Šä¸€æ¬¡AIå›å¤çš„å†…å®¹
                st.session_state['edit_answer'] = st.session_state["conv_robot"][-1]
                st.text_area('å¯¹è¯å†…å®¹', key='edit_answer', height=800)
                st.form_submit_button("ğŸ“ ç¡®è®¤ä¿®æ”¹", onclick=edit_answer)
    else:
        st.warning("æ— æ³•ç¼–è¾‘ï¼å¯¹è¯ä¸å­˜åœ¨")

def edit_answer():
    # ä¿®æ”¹ä¸Šä¸€æ¬¡å¯¹è¯å†…å®¹
    if len(st.session_state["conv_robot"]) > 0:
        txt = st.session_state['edit_answer']
        st.session_state["conv_robot"][-1] = txt
        st.success("å¯¹è¯å†…å®¹å·²ä¿®æ”¹")


def rollback():
    # ç§»é™¤æœ€æ–°çš„ä¸€è½®å¯¹è¯
    st.session_state['conv_robot'].pop()
    user_input = st.session_state['conv_user'].pop()
    st.write('robot invoke', user_input)
    st.session_state['input'] = user_input


preset_id_options = [p["preset"] for p in PROMPTS]
preset_id_options.append("è‡ªå®šä¹‰")
if 'preset' not in st.session_state:
    load_preset_qa()
prompt_id = st.sidebar.selectbox('é¢„è®¾èº«ä»½çš„æç¤ºè¯', options=preset_id_options, index=0, on_change=load_preset_qa, key="preset")
_prompt_text = get_prompt_by_preset_id(prompt_id)
prompt_text = st.sidebar.text_area("Enter Prompt", value=_prompt_text, placeholder='é¢„è®¾çš„Prompt', 
                            label_visibility='collapsed', key='prompt_system', disabled=(_prompt_text != ''))
st.session_state.input_text_state = prompt_text
append_to_input_text()
need_edit_answer = st.sidebar.button("ç¼–è¾‘AIçš„å›ç­”ï¼ˆé«˜çº§åŠŸèƒ½ï¼‰")
if need_edit_answer:
    show_edit_dialog()
    
with st.form("my_form"):
    col_icon, col_text, col_btn = st.columns((1, 10, 2))
    col_icon.markdown(f"""<img src="https://api.dicebear.com/5.x/{"lorelei"}/svg?seed={seed}" alt="avatar" />""", unsafe_allow_html=True)
    input_text = col_text.text_input("You: ", "", key="input", label_visibility="collapsed")

    model_val = st.sidebar.selectbox("Model", options=LANGUAGE_MODELS, index=0)
    temperature_val = st.sidebar.slider("Temperature", 0.0, 2.0, 0.9, step=0.05)
    max_tokens_val = st.sidebar.select_slider("Max Tokens", options=(256, 512, 1024), value=512) 
    # Every form must have a submit button.
    submitted = col_btn.form_submit_button("ğŸ’¬")
    if submitted:
        response, answer = after_submit(input_text, model_val, temperature_val, max_tokens_val)
        st.session_state.conv_user.append(input_text)
        st.session_state.conv_robot.append(answer)
    
    show_conversation_dialog(rollback_fn=rollback)

    # When the input_text_state is bind to widget, its content cannot be modified by session api.
    with st.expander(""):
        st.json(st.session_state.conv_robot, expanded=False)
        st.json(st.session_state.conv_user, expanded=False)
        txt = st.text_area('å¯¹è¯å†…å®¹', key='input_text_state', height=800)
    tokens = get_tokenizer().tokenize(txt)
    token_number = len(tokens)
    st.write('å…¨æ–‡çš„ Token æ•°ï¼š', token_number, ' ï¼ˆæœ€å¤§ Token æ•°ï¼š`4096`ï¼‰')
    if submitted:
        st.json(response, expanded=False)
        # st.write("temperature", temperature_val)

"""---"""

with st.expander("è®¿é—®ç»Ÿè®¡"):
    pv_stats, call_stats, token_stats = get_token_counter().summary()

    tab1, tab2, tab3 = st.tabs(["Session View", "Request Count", "Token Count"])
    df = pd.DataFrame(pv_stats.items(), columns=['time', 'pv'])
    fig = px.line(df, x="time", y="pv", title='Page View')
    tab1.plotly_chart(fig)
    tab2.write(call_stats)
    tab3.write(token_stats)
