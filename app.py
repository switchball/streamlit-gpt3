import time
import random
import openai
import streamlit as st
import pandas as pd
import plotly.express as px
from utils.common_resource import get_tokenizer


from prompt import PROMPTS, get_prompt_by_preset_id, get_suggestion_by_preset_id, get_description_by_preset_id
from collect import TokenCounter
from dialog import message
from share import generate_share_link, restore_from_share_link
from image import conversation2png


openai.api_key = st.secrets["OPENAI_API_KEY"]


st.set_page_config(
    page_title="GPT-3 Playground", layout="wide", initial_sidebar_state="auto",
)

st_tiltle_slot = st.empty()
st_tiltle_slot.title("GPT-3 ä½ é—®æˆ‘ç­”")
st.markdown("""[![GitHub][github_badge]][github_link]\n\n[github_badge]: https://badgen.net/badge/icon/GitHub?icon=github&color=black&label\n[github_link]: https://github.com/switchball/streamlit-gpt3""")
st_desc_solt = st.empty()
st_desc_solt.text("åœ¨ä¸‹æ–¹æ–‡æœ¬æ¡†è¾“å…¥ä½ çš„å¯¹è¯ âœ¨ æ”¯æŒå¤šè½®å¯¹è¯ ğŸ˜‰ \nç‚¹å‡» \"ğŸ’¬\" åï¼Œç¨ç­‰ç‰‡åˆ»ï¼Œå°±ä¼šæ”¶åˆ°æ¥è‡ª GPT-3 çš„å›å¤")

# st.success('GPT-3 éå¸¸æ“…é•¿ä¸äººå¯¹è¯ï¼Œç”šè‡³æ˜¯ä¸è‡ªå·±å¯¹è¯ã€‚åªéœ€è¦å‡ è¡Œçš„æŒ‡ç¤ºï¼Œå°±å¯ä»¥è®© AI æ¨¡ä»¿å®¢æœèŠå¤©æœºå™¨äººçš„è¯­æ°”è¿›è¡Œå¯¹è¯ã€‚\nå…³é”®åœ¨äºï¼Œéœ€è¦æè¿° AI åº”è¯¥è¡¨ç°æˆä»€ä¹ˆæ ·ï¼Œå¹¶ä¸”ä¸¾å‡ ä¸ªä¾‹å­ã€‚', icon="âœ…")
# st.success('çœ‹èµ·æ¥å¾ˆç®€å•ï¼Œä½†ä¹Ÿæœ‰äº›éœ€è¦é¢å¤–æ³¨æ„çš„åœ°æ–¹ï¼š\n1. åœ¨å¼€å¤´æè¿°æ„å›¾ï¼Œä¸€å¥è¯æ¦‚æ‹¬ AI çš„ä¸ªæ€§ï¼Œé€šå¸¸è¿˜éœ€è¦ 1~2 ä¸ªä¾‹å­ï¼Œæ¨¡ä»¿å¯¹è¯çš„å†…å®¹ã€‚\n2. ç»™ AI ä¸€ä¸ªèº«ä»½(identity)ï¼Œå¦‚æœæ˜¯ä¸ªåœ¨å®éªŒå®¤ç ”ç©¶çš„ç§‘å­¦å®¶èº«ä»½ï¼Œé‚£å¯èƒ½å°±ä¼šå¾—åˆ°æ›´æœ‰æ™ºæ…§çš„è¯ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¯å‚è€ƒçš„ä¾‹å­', icon="âœ…")

st.write('''<style>
[data-testid="column"] {
    min-width: 1rem !important;
}
</style>''', unsafe_allow_html=True)

@st.cache_resource
def get_token_counter():
    # if the definition of TokenCounter changes, the app need to reboot.
    tc = TokenCounter(interval=900)
    return tc


def wait(delay, reason=""):
    if delay <= 50:
        return
    end = time.time() + delay
    for t in range(int(delay)):
        with st.spinner(text=f"{reason} é¢„è®¡ç­‰å¾…æ—¶é—´ {round(end - time.time())} ç§’"):
            time.sleep(random.uniform(2, 7))
        if time.time() > end:
            break


@st.cache_data(ttl=3600)
def completion(
        prompt, 
        model="text-davinci-003",
        temperature=0.9,
        max_tokens=256,
        top_p=1,
        frequency_penalty=0,
        presence_penalty=0.6,
        stop=[" Human:", " AI:"]
    ):
    """ Text completion """
    print('completion', prompt)
    with st.spinner(text=random.choice(HINT_TEXTS)):
        response = openai.Completion.create(
            model=model, prompt=prompt, temperature=temperature, max_tokens=max_tokens, top_p=top_p, 
            frequency_penalty=frequency_penalty, presence_penalty=presence_penalty, stop=stop
        )
    print('completion finished.')
    print(response['choices'][0]['text'])
    return response


@st.cache_data(ttl=3600)
def chat_completion(
    message_list,
    model="gpt-3.5-turbo",
    temperature=0.9,
    max_tokens=256,
    top_p=1,
    frequency_penalty=0,
    presence_penalty=0.6,
    stream=False
    ):
    """ Chat completion """
    with st.spinner(text=random.choice(HINT_TEXTS)):
        response = openai.ChatCompletion.create(
            model=model, messages=message_list, temperature=temperature, max_tokens=max_tokens, top_p=top_p, 
            frequency_penalty=frequency_penalty, presence_penalty=presence_penalty, stream=stream
        )
    if stream:
        reply_msg = ""
        finish_reason = ""

        # streaming chat with editable slot
        reply_edit_slot = st.empty()
        for chunk in response:
            c = chunk['choices'][0]
            delta = c.get('delta', {}).get('content', '')
            finish_reason = c.get('finish_reason', '')
            reply_msg += delta
            reply_edit_slot.markdown(reply_msg)
        reply_edit_slot.markdown("")

        # calculate message tokens
        txt = "".join(m["content"] for m in message_list)
        input_tokens = len(get_tokenizer().tokenize(txt))
        completion_tokens = len(get_tokenizer().tokenize(reply_msg))

        # mock response
        response = {
            'choices': [{
                'message': {
                    'content': reply_msg,
                    'role': 'assistant'
                },
                'finish_reason': finish_reason
            }],
            'usage': {
                'total_tokens': input_tokens + completion_tokens
            }
        }
        return response
    else:
        return response

# Available Models
LANGUAGE_MODELS = ['gpt-3.5-turbo', 'text-davinci-003', 'text-curie-001', 'text-babbage-001', 'text-ada-001']
CODEX_MODELS = ['code-davinci-002', 'code-cushman-001']

HINT_TEXTS = ['æ­£åœ¨æ¥é€šç”µæºï¼Œè¯·ç¨ç­‰ ...', 'æ­£åœ¨æ€è€ƒæ€ä¹ˆå›ç­”ï¼Œä¸è¦ç€æ€¥', 'æ­£åœ¨åŠªåŠ›æŸ¥è¯¢å­—å…¸å†…å®¹ ...', 'ç­‰å¾…å¯¹æ–¹å›å¤ä¸­ ...', 'æ­£åœ¨æ¿€æ´»ç¥ç»ç½‘ç»œ ...', 'è¯·ç¨ç­‰']
TOKEN_SAVING_HINT_THRESHOLD = 2000

# store chat as session state
DEFAULT_CHAT_TEXT = "ä»¥ä¸‹æ˜¯ä¸AIåŠ©æ‰‹çš„å¯¹è¯ã€‚åŠ©æ‰‹ä¹äºåŠ©äººã€æœ‰åˆ›æ„ã€èªæ˜è€Œä¸”éå¸¸å‹å¥½ã€‚\n\n"

if 'input_text_state' not in st.session_state:
    st.session_state.input_text_state = DEFAULT_CHAT_TEXT

if 'conv_user' not in st.session_state:
    st.session_state.conv_user = []

if 'conv_robot' not in st.session_state:
    st.session_state.conv_robot = []

if 'user' not in st.session_state:
    st.session_state['user'] = 'new user'
    get_token_counter().page_view()

if 'seed' not in st.session_state:
    st.session_state['seed'] = random.randint(0, 1000)
seed = st.session_state['seed']


class ConversationCompressConfig:
    def __init__(self, *, enabled, max_human_conv_reserve_count=None, max_robot_conv_reserve_count=None, enable_first_conv=None) -> None:
        self.enabled = enabled
        self.max_human_conv_reserve_count = max_human_conv_reserve_count
        self.max_robot_conv_reserve_count = max_robot_conv_reserve_count
        self.enable_first_conv = enable_first_conv
    
    def get_message_list(self):
        if self.enabled:
            return self._get_compressed_message_list()
        else:
            return self._get_full_message_list()
    
    @property
    def message_tokens(self):
        if self.enabled:
            return self.compressed_message_tokens
        else:
            return self.full_message_tokens
    
    @property
    def full_message_tokens(self):
        ms = self._get_full_message_list()
        txt = "".join(m["content"] for m in ms)
        tokens = get_tokenizer().tokenize(txt)
        return len(tokens)
    
    @property
    def compressed_message_tokens(self):
        ms = self._get_compressed_message_list()
        txt = "".join(m["content"] for m in ms)
        tokens = get_tokenizer().tokenize(txt)
        return len(tokens)

    def _get_full_message_list(self):
        """ Get full message list (for Chat Completion) """
        message_list = []
        # Add system prompt
        if st.session_state['prompt_system']:
            message_list.append({"role": "system", "content": st.session_state["prompt_system"]})
        # Add history conversations
        for conv_user, conv_robot in zip(st.session_state['conv_user'], st.session_state['conv_robot']):
            message_list.append({"role": "user", "content": conv_user})
            message_list.append({"role": "assistant", "content": conv_robot})
        return message_list

    def _get_compressed_message_list(self):
        """ Get compressed message list (for Chat Completion) """
        message_list = []
        # Add system prompt
        if st.session_state['prompt_system']:
            message_list.append({"role": "system", "content": st.session_state["prompt_system"]})
        # Add history conversations but compressed
        turns_count = min(len(st.session_state['conv_user']), len(st.session_state['conv_robot']))
        for turn_idx in range(turns_count):
            should_keep_human = False   # should keep human conversations at this turn
            should_keep_robot = False   # should keep robot conversations at this turn
            if turn_idx == 0 and self.enable_first_conv:
                should_keep_human, should_keep_robot = True, True
            if turn_idx + self.max_human_conv_reserve_count >= turns_count:
                should_keep_human = True
            if turn_idx + self.max_robot_conv_reserve_count >= turns_count:
                should_keep_robot = True
            # Add conversations to message_list
            if should_keep_human or should_keep_robot:
                conv_user = st.session_state['conv_user'][turn_idx] if should_keep_human else ""
                conv_robot = st.session_state['conv_robot'][turn_idx] if should_keep_robot else ""
                message_list.append({"role": "user", "content": conv_user})
                message_list.append({"role": "assistant", "content": conv_robot})
            
        return message_list

def after_submit(current_input, model, temperature, max_tokens, cc_config: ConversationCompressConfig, stream=False):
    # Append current_input to input_text_state
    st.session_state.input_text_state += current_input

    # Queue by prompt length and max_tokens
    if 'gpt' in model:
        token_number = cc_config.message_tokens
    else:
        token_number = len(get_tokenizer().tokenize(st.session_state.input_text_state))
    x = token_number / 1024
    delay = 2 * x * x - 3
    delay += 2 * x * (max_tokens / 1024 - 1)
    wait(delay, "å‰æ–¹æ’é˜Ÿä¸­...")

    # Send text and waiting for respond
    if 'gpt' in model:
        # Get system prompt + history conversations
        message_list = cc_config.get_message_list()
        # Add current user input
        message_list.append({"role": "user", "content": current_input})
        response = chat_completion(
            message_list=message_list,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            top_p=1,
            frequency_penalty=0,
            presence_penalty=0.6,
            stream=stream
        )
        answer = response['choices'][0]['message']['content']
        st.session_state.input_text_state += answer
    else:
        response = completion(
            model=model,
            prompt=st.session_state.input_text_state,
            temperature=temperature,
            max_tokens=max_tokens,
            top_p=1,
            frequency_penalty=0,
            presence_penalty=0.6,
            stop=[" Human:", " AI:"]
        )
        # TODO: non chat model also need stream function
        answer = response['choices'][0]['text']
        # TODO: should check if answer starts with '\nAI:'
        st.session_state.input_text_state += answer
        st.session_state.input_text_state += '\nHuman: '

    print(answer)
    # Collect usage
    tc = get_token_counter()
    tc.collect(tokens=response['usage']['total_tokens'])
    return response, answer


def load_preset_id_from_url_link():
    """ Load preset if it is provided in url link """
    query = st.experimental_get_query_params()
    preset_id = query.get("preset", [""])[0]
    if preset_id:
        for p in PROMPTS:
            if preset_id == p['preset']:
                return preset_id
    return None

def load_preset_qa(candidate=None):
    """ Load default preset Q&A """
    preset = st.session_state.get("preset", candidate or 'GPT-3 ä½ é—®æˆ‘ç­” (ChatBot)')
    st.session_state['conv_user'].clear()
    st.session_state['conv_robot'].clear()
    # load prompt message into conversations
    for p in PROMPTS:
        if preset == p['preset']:
            st.session_state['input'] = p.get('input', '')
            for message in p['message']:
                if message['role'] == 'user':
                    st.session_state['conv_user'].append(message['content'])
                elif message['role'] == 'assistant':
                    st.session_state['conv_robot'].append(message['content'])


def append_to_input_text():
    """ Restore input_text_state via chat history """
    if st.session_state.conv_robot:
        st.session_state.input_text_state += '\nHuman: '
        for i in range(len(st.session_state.conv_robot)):
            st.session_state.input_text_state += st.session_state['conv_user'][i]
            st.session_state.input_text_state += st.session_state["conv_robot"][i]
            st.session_state.input_text_state += '\nHuman: '


def show_conversation_dialog(slot_list, rollback_fn, reverse_order=False):
    """ Render the conversation dialogs """
    just_loaded_from_share = False
    if 'loaded_from_share' in st.session_state and st.session_state['loaded_from_share']:
        just_loaded_from_share = True
        st.session_state['loaded_from_share'] = False
    if not slot_list:
        reverse_order = True
    if st.session_state.conv_robot:
        num = len(st.session_state.conv_robot)
        # From user0, robot0, ..., user_{n-1}, robot_{n-1} in normal order
        order_indexes = reversed(range(2*num)) if reverse_order else range(2*num)
        for j in order_indexes:
            slot = st.empty() if reverse_order else slot_list[j]
            with slot:
                is_user = j % 2 == 0
                text = st.session_state['conv_user'][j//2] if is_user else st.session_state["conv_robot"][j//2]
                message(text, is_user=is_user, key=str(j), seed=seed, on_click=(rollback_fn if j == 2 * num - 1 else None))
            if just_loaded_from_share:
                time.sleep(1)
                
def show_edit_dialog(slot):
    """ Show dialog that edits AI answer """
    with slot:
        if len(st.session_state["conv_robot"]) > 0:
            with st.expander("â­ æ‰‹åŠ¨ç¼–è¾‘ä¸Šä¸€æ¬¡AIå›å¤çš„å†…å®¹", expanded=True):
                with st.form("edit_form"):
                    # åŠ è½½ä¸Šä¸€æ¬¡AIå›å¤çš„å†…å®¹
                    st.session_state['edit_answer'] = st.session_state["conv_robot"][-1]
                    st.form_submit_button("ğŸ“ ç¡®è®¤ä¿®æ”¹", on_click=edit_answer)
                    st.text_area('å¯¹è¯å†…å®¹', key='edit_answer', height=800)
        else:
            st.warning("æ— æ³•ç¼–è¾‘ï¼å¯¹è¯ä¸å­˜åœ¨")

def edit_answer():
    # ä¿®æ”¹ä¸Šä¸€æ¬¡å¯¹è¯å†…å®¹
    if len(st.session_state["conv_robot"]) > 0:
        txt = st.session_state['edit_answer']
        st.session_state["conv_robot"][-1] = txt
        st.success("å¯¹è¯å†…å®¹å·²ä¿®æ”¹")


def rollback():
    # ç§»é™¤æœ€æ–°çš„ä¸€è½®å¯¹è¯
    st.session_state['conv_robot'].pop()
    user_input = st.session_state['conv_user'].pop()
    st.write('robot invoke', user_input)
    st.session_state['input'] = user_input


# æ¢å¤ / ä¿å­˜
restore_from_share_link()

st.sidebar.title("âœ¨ GPT Interface")
with st.sidebar.expander('ğŸˆ é¢„è®¾èº«ä»½çš„æç¤ºè¯ (Preset Prompts)', expanded=False):
    preset_id_options = [p["preset"] for p in PROMPTS]
    preset_id_options.append("è‡ªå®šä¹‰")
    if 'preset' not in st.session_state:
        load_preset_qa(candidate=load_preset_id_from_url_link())
    prompt_id = st.selectbox('é¢„è®¾èº«ä»½çš„æç¤ºè¯', options=preset_id_options, index=0, on_change=load_preset_qa, key="preset", label_visibility="collapsed")
    # åŠ¨æ€æ›´æ”¹æ ‡é¢˜å’Œè¯´æ˜
    st_tiltle_slot.title(prompt_id)
    if get_description_by_preset_id(prompt_id) is not None:
        st_desc_solt.text(get_description_by_preset_id(prompt_id))
    _prompt_text = get_prompt_by_preset_id(prompt_id)
    prompt_text = st.text_area("Enter Prompt", value=_prompt_text, placeholder='é¢„è®¾çš„Prompt', 
                                label_visibility='collapsed', key='prompt_system', disabled=(_prompt_text != ''))
    _suggestion = get_suggestion_by_preset_id(prompt_id)
    if _suggestion:
        st.warning(_suggestion, icon="âš ï¸")
    st.session_state.input_text_state = prompt_text
    append_to_input_text()

edit_answer_slot = st.empty()

# å¯¹è¯ä¿ç•™è®¾ç½®
with st.sidebar.expander('â­ å¯¹è¯è®¾ç½®'):
    enbale_conv_reserve = st.checkbox("å¼€å¯å¯¹è¯å‹ç¼©", value=False, help="è‹¥å¼€å¯ï¼Œä»…ä¼šå‘é€å¯¹è¯ä¸­çš„ç‰¹å®šéƒ¨åˆ†ä½œä¸ºä¸Šä¸‹æ–‡\n\nè‹¥å…³é—­ï¼Œæ‰€æœ‰èŠå¤©å†…å®¹éƒ½ä¼šä½œä¸ºä¸Šä¸‹æ–‡å‘é€")
    if enbale_conv_reserve:
        max_robot_conv_reserve_count = st.number_input(':hash: ä»…ä¿ç•™æœ€è¿‘AIå›å¤å¯¹è¯æ•°', 0, None, 3, help="è®¾å®šæœ€å¤šä¿ç•™å¤šå°‘æ¬¡ AI æœ€è¿‘çš„å›å¤å†…å®¹")
        max_human_conv_reserve_count = st.number_input(':hash: ä»…ä¿ç•™æœ€è¿‘è¾“å…¥å¯¹è¯æ•°', 0, None, 10, help="è®¾å®šæœ€å¤šä¿ç•™å¤šå°‘æ¬¡æœ€è¿‘è¾“å…¥çš„æé—®å†…å®¹")
        enable_first_conv = st.checkbox('å¿…å®šä¿ç•™ç¬¬ä¸€è½®å¯¹è¯', help="æ¨èåœ¨ç¬¬ä¸€è½®å¯¹è¯åŒ…å«ç‰¹æ®Šè®¾å®šæ—¶å¼€å¯")

        cc_config = ConversationCompressConfig(
            enabled=True,
            max_human_conv_reserve_count=max_human_conv_reserve_count,
            max_robot_conv_reserve_count=max_robot_conv_reserve_count,
            enable_first_conv=enable_first_conv)
        full_tokens = cc_config.full_message_tokens
        active_tokens = cc_config.compressed_message_tokens
        st.caption(f"é¢„ä¼°å‹ç¼©å‰/åï¼š `{active_tokens}`/ `{full_tokens}` tokens")
    else:
        cc_config = ConversationCompressConfig(enabled=False)
    enable_reverse_order = st.checkbox("å¯¹è¯å€’åºæ˜¾ç¤º", value=False, help="å¼€å¯åï¼Œè¾“å…¥æ¡†åœ¨ä¸Šæ–¹ï¼Œæœ€è¿‘çš„å¯¹è¯åœ¨æœ€ä¸Šæ–¹\n\nå…³é—­åï¼Œè¾“å…¥æ¡†åœ¨ä¸‹æ–¹ï¼Œæœ€æ—©çš„å¯¹è¯åœ¨ä¸Šæ–¹")
    enable_stream_chat = st.checkbox("å¯¹è¯æµå¼æ˜¾ç¤º", value=True, help="å¼€å¯åï¼Œä»¥æµå¼æ–¹å¼ä¼ è¾“å¯¹è¯ï¼Œæ— éœ€ç­‰å¾…")

if st.session_state['input_text_state'] and not enbale_conv_reserve:
    tokens = get_tokenizer().tokenize(st.session_state['input_text_state'])
    if len(tokens) > TOKEN_SAVING_HINT_THRESHOLD:
        st.sidebar.info(f"ğŸ‘† å…¨æ–‡ Token æ•° >= {TOKEN_SAVING_HINT_THRESHOLD}ï¼Œå¯è€ƒè™‘å¼€å¯å¯¹è¯å‹ç¼©åŠŸèƒ½")


with st.form("my_form"):
    dialog_slot_list = None if enable_reverse_order else [st.empty() for _ in range(2 + 2 * len(st.session_state['conv_user']))]
    col_icon, col_text, col_btn = st.columns((1, 10, 2))
    col_icon.markdown(f"""<img src="https://api.dicebear.com/5.x/{"lorelei"}/svg?seed={seed}" alt="avatar" />""", unsafe_allow_html=True)
    input_text = col_text.text_input("You: ", "", key="input", label_visibility="collapsed")

    with st.sidebar.expander('ğŸ§© æ¨¡å‹å‚æ•° (Model Parameters)'):
        model_val = st.selectbox("Model", options=LANGUAGE_MODELS, index=0)
        temperature_val = st.slider("Temperature", 0.0, 2.0, 0.9, step=0.05)
        max_tokens_val = st.select_slider("Max Tokens", options=(256, 512, 1024), value=512) 
    # Every form must have a submit button.
    submitted = col_btn.form_submit_button("ğŸ’¬")
    if submitted:
        response, answer = after_submit(input_text, model_val, temperature_val, max_tokens_val, cc_config, stream=enable_stream_chat)
        st.session_state.conv_user.append(input_text)
        st.session_state.conv_robot.append(answer)
        finish_reason = response['choices'][0].get('finish_reason', '')
        if finish_reason == 'length':
            st.sidebar.info("ğŸ‘† ä¸Šæ¬¡è¾“å…¥å› é•¿åº¦è¢«æˆªæ–­ï¼Œå¯è€ƒè™‘æ’¤å›è¯¥æ¶ˆæ¯ï¼Œå¹¶è°ƒå¤§è¯¥å‚æ•°åé‡è¯•")
    
    show_conversation_dialog(dialog_slot_list, rollback_fn=rollback, reverse_order=enable_reverse_order)

    # When the input_text_state is bind to widget, its content cannot be modified by session api.
    with st.expander(""):
        st.json(st.session_state.conv_robot, expanded=False)
        st.json(st.session_state.conv_user, expanded=False)
        txt = st.text_area('å¯¹è¯å†…å®¹', key='input_text_state', height=800)
    tokens = get_tokenizer().tokenize(txt)
    token_number = len(tokens)
    st.write('å…¨æ–‡çš„ Token æ•°ï¼š', token_number, ' ï¼ˆæœ€å¤§ Token æ•°ï¼š`4096`ï¼‰')
    if submitted:
        st.json(response, expanded=False)
        # st.write("temperature", temperature_val)

need_edit_answer = st.sidebar.button("ğŸ”¬ ç¼–è¾‘AIçš„å›ç­”ï¼ˆé«˜çº§åŠŸèƒ½ï¼‰")
if need_edit_answer:
    show_edit_dialog(slot=edit_answer_slot)
    
# æ¢å¤ / ä¿å­˜
if st.sidebar.button("ğŸ”— ç”Ÿæˆåˆ†äº«é“¾æ¥"):
    share_link = generate_share_link()
    st.sidebar.success(f"é“¾æ¥å·²ç”Ÿæˆ [å³é”®å¤åˆ¶]({share_link}) æœ‰æ•ˆæœŸ7å¤©")

is_generate_image = st.sidebar.button("ğŸ–¼ï¸ ç”Ÿæˆåˆ†äº«å›¾ç‰‡", key="image_button")
if is_generate_image:
    image = conversation2png(st.session_state['preset'], st.session_state['conv_user'], st.session_state['conv_robot'], seed=seed)
    st.image(image, caption='å·²ç”Ÿæˆå›¾ç‰‡ï¼Œé•¿æŒ‰æˆ–å³é”®ä¿å­˜')

"""---"""

with st.expander("è®¿é—®ç»Ÿè®¡"):
    pv_stats, call_stats, token_stats = get_token_counter().summary()

    tab1, tab2, tab3 = st.tabs(["Session View", "Request Count", "Token Count"])
    df = pd.DataFrame(pv_stats.items(), columns=['time', 'pv'])
    fig = px.line(df, x="time", y="pv", title='Page View')
    tab1.plotly_chart(fig)
    tab2.write(call_stats)
    tab3.write(token_stats)
